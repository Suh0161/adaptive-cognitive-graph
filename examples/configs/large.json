{
  "d_model": 4096,
  "n_heads": 64,
  "n_layers": 16,
  "vocab_size": 50257,
  "max_seq_len": 256000,
  "ssm_every": 2,
  "ffn_mult": 4,
  "activation": "swiglu",
  "dropout": 0.05,
  "n_experts": 64,
  "active_experts": 8,
  "n_phases": 8,
  "pooling": "attention",
  "expert_layers": 6,
  "enable_cross_edges": true,
  "dag_topology": "phase_grouped",
  "weight_tying": false,
  "expert_parallel": true,
  "use_memory_adapter": true,
  "lora_rank": 32,
  "lora_alpha": 0.3,
  "ema_decay": 0.92,
  "verifier_hidden": 2048,
  "verifier_layers": 3,
  "verifier_threshold": 0.5,
  "correction_type": "transformer",
  "balance_loss_weight": 0.005,
  "verify_loss_weight": 0.03,
  "phase_loss_weight": 0.01,
  "grad_clip": 1.0,
  "learning_rate": 5e-05,
  "optimizer": "adamw",
  "batch_size": 4,
  "use_gradient_checkpointing": true,
  "mixed_precision": "bf16",
  "_comment": "Large ACG model configuration (~100B+ parameters)",
  "_description": "Large-scale configuration for maximum performance. Uses 64 experts with 8 active per token, providing extensive specialization while maintaining <40B active parameters. Supports extreme context lengths up to 256K tokens. Requires large-scale distributed training infrastructure. Designed to scale beyond 1T parameters with sparse activation.",
  "_estimated_params": "~100B-1T+ total, ~20-35B active per token",
  "_active_params_constraint": "Maintains <40B active parameters per token",
  "_recommended_hardware": "32-128 GPUs with 80GB VRAM each (H100, A100 80GB)",
  "_training_tips": [
    "Requires DeepSpeed ZeRO-3 or FSDP for training",
    "Use expert parallelism to distribute experts across nodes",
    "Enable tensor parallelism for large layers",
    "Pipeline parallelism for layer distribution",
    "Batch size of 4 per GPU with extensive gradient accumulation",
    "Supports extreme context lengths up to 256K tokens",
    "Training time: Several weeks to months on large cluster",
    "Consider using Flash Attention 2 for memory efficiency",
    "Use gradient checkpointing to fit in memory",
    "Monitor expert utilization to ensure balanced routing"
  ],
  "_hyperparameter_notes": {
    "d_model": "4096 for maximum model capacity",
    "n_layers": "16 encoder layers with SSM blocks for long-context",
    "n_experts": "64 experts for fine-grained specialization",
    "active_experts": "8 active keeps compute under 40B params",
    "n_phases": "8 phases for detailed cognitive processing stages",
    "expert_layers": "6 layers per expert for deep processing",
    "pooling": "Attention-based pooling for better routing decisions",
    "ssm_every": "SSM blocks every 2 layers for linear-time long-context",
    "dropout": "Lower dropout (0.05) for large models",
    "learning_rate": "Very low LR (5e-5) for stable large-scale training",
    "lora_rank": "Higher rank (32) for more adaptation capacity",
    "verifier_hidden": "Large verifier (2048) for thorough quality checks"
  },
  "_scaling_notes": {
    "total_params_estimate": "~100B base + experts can scale to 1T+",
    "active_params_per_token": "~20-35B (well under 40B requirement)",
    "expert_params": "Each expert ~1-2B params, 64 experts total",
    "encoder_params": "~15-20B for 16-layer encoder",
    "sparsity_ratio": "~8/64 = 12.5% expert activation",
    "memory_footprint": "~200-400GB model weights (fp16/bf16)",
    "training_memory": "~1-2TB with optimizer states and gradients"
  },
  "_performance_targets": {
    "context_length": "Up to 256K tokens with linear complexity",
    "throughput": "~1-2K tokens/sec on 32xH100",
    "latency": "~100-200ms per forward pass (batch=1)",
    "expert_utilization": ">80% of experts used over 1000 steps",
    "routing_entropy": ">0.8 (normalized) for diverse routing"
  }
}
