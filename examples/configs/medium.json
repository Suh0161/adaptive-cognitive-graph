{
  "d_model": 2048,
  "n_heads": 32,
  "n_layers": 8,
  "vocab_size": 50257,
  "max_seq_len": 32768,
  "ssm_every": 2,
  "ffn_mult": 4,
  "activation": "swiglu",
  "dropout": 0.1,
  "n_experts": 32,
  "active_experts": 4,
  "n_phases": 4,
  "pooling": "mean",
  "expert_layers": 4,
  "enable_cross_edges": true,
  "dag_topology": "phase_grouped",
  "weight_tying": false,
  "expert_parallel": true,
  "use_memory_adapter": true,
  "lora_rank": 16,
  "lora_alpha": 0.2,
  "ema_decay": 0.9,
  "verifier_hidden": 512,
  "verifier_layers": 2,
  "verifier_threshold": 0.5,
  "correction_type": "transformer",
  "balance_loss_weight": 0.01,
  "verify_loss_weight": 0.05,
  "phase_loss_weight": 0.02,
  "grad_clip": 1.0,
  "learning_rate": 0.0001,
  "optimizer": "adamw",
  "batch_size": 16,
  "use_gradient_checkpointing": true,
  "mixed_precision": "bf16",
  "_comment": "Medium ACG model configuration (~7B parameters)",
  "_description": "Balanced configuration for production use. Uses 32 experts with 4 active per token, providing good capacity while maintaining efficiency. Enables cross-edge communication for expert collaboration. Supports long contexts up to 32K tokens with SSM blocks. Requires multi-GPU setup for training.",
  "_estimated_params": "~7B total, ~1.5B active per token",
  "_recommended_hardware": "4-8 GPUs with 40GB+ VRAM each (A100, H100)",
  "_training_tips": [
    "Use expert_parallel=true to distribute experts across GPUs",
    "Enable gradient checkpointing to reduce memory usage",
    "Batch size of 16 per GPU with gradient accumulation",
    "Suitable for sequence lengths up to 32K tokens",
    "Training time: ~1-2 weeks on 8xA100 for large dataset",
    "Consider using DeepSpeed ZeRO-3 for memory optimization"
  ],
  "_hyperparameter_notes": {
    "d_model": "2048 provides good balance between capacity and efficiency",
    "n_experts": "32 experts allow for diverse specialization",
    "active_experts": "4 active experts per token keeps compute manageable",
    "n_phases": "4 phases (e.g., plan, reason, verify, summarize)",
    "expert_layers": "4 layers per expert for sufficient capacity",
    "ssm_every": "Insert SSM block every 2 layers for long-context stability",
    "learning_rate": "Lower LR (1e-4) for stable training at this scale"
  }
}
