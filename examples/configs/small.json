{
  "d_model": 512,
  "n_heads": 8,
  "n_layers": 4,
  "vocab_size": 50257,
  "max_seq_len": 2048,
  "ssm_every": 2,
  "ffn_mult": 4,
  "activation": "swiglu",
  "dropout": 0.1,
  "n_experts": 8,
  "active_experts": 2,
  "n_phases": 2,
  "pooling": "mean",
  "expert_layers": 2,
  "enable_cross_edges": false,
  "dag_topology": "phase_grouped",
  "weight_tying": true,
  "expert_parallel": false,
  "use_memory_adapter": true,
  "lora_rank": 8,
  "lora_alpha": 0.2,
  "ema_decay": 0.9,
  "verifier_hidden": 256,
  "verifier_layers": 2,
  "verifier_threshold": 0.5,
  "correction_type": "mlp",
  "balance_loss_weight": 0.01,
  "verify_loss_weight": 0.05,
  "phase_loss_weight": 0.02,
  "grad_clip": 1.0,
  "learning_rate": 0.0003,
  "optimizer": "adamw",
  "batch_size": 32,
  "use_gradient_checkpointing": false,
  "mixed_precision": "bf16",
  "_comment": "Small ACG model configuration (~50M parameters)",
  "_description": "Suitable for experimentation and quick prototyping. Uses minimal experts (8) with 2 active per token. Lightweight expert blocks (2 layers) and disabled cross-edge communication for efficiency. Weight tying enabled to reduce parameters. Good for single GPU training.",
  "_estimated_params": "~50M total, ~15M active per token",
  "_recommended_hardware": "Single GPU with 8GB+ VRAM",
  "_training_tips": [
    "Use batch_size=32-64 for optimal throughput",
    "Can train on consumer GPUs (RTX 3090, 4090)",
    "Suitable for sequence lengths up to 2K tokens",
    "Training time: ~1-2 days on single GPU for small dataset"
  ]
}
